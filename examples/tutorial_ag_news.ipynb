{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic ID Tutorial: Text Clustering with AG News\n",
    "\n",
    "Welcome to this tutorial on **Semantic ID**! \n",
    "\n",
    "In this notebook, we will demonstrate how to generate semantic identifiers for a textual dataset (AG News). We will cover:\n",
    "\n",
    "1.  **RQ-KMeans**: Hierarchical clustering on CPU, GPU, and MPS.\n",
    "2.  **Uniqueness**: Handling ID collisions automatically.\n",
    "3.  **RQ-VAE**: Using neural networks for quantization.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "First, let's install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install semantic-id datasets sentence-transformers pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhail/VSCodeProjects/semantic-id/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e1516d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Semantic ID imports\n",
    "from semantic_id.algorithms.rq_kmeans import RQKMeans\n",
    "from semantic_id.algorithms.rq_vae import RQVAE\n",
    "from semantic_id.engine import SemanticIdEngine\n",
    "from semantic_id.uniqueness.resolver import UniqueIdResolver\n",
    "from semantic_id.uniqueness.stores import SQLiteCollisionStore\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Generate Embeddings\n",
    "\n",
    "We will use the **AG News** dataset, which contains news articles categorized into World, Sports, Business, and Sci/Tech. This is a perfect dataset to see if our IDs capture semantic meaning (e.g., all Sports articles should have similar IDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AG News dataset...\n",
      "Loaded 20000 samples.\n",
      "Example category: World\n",
      "Example text: Bangladesh paralysed by strikes Opposition activists have brought many towns and cities in Banglades...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "\n",
    "# Subsample for this tutorial (20,000 samples)\n",
    "# This keeps the execution fast while providing enough data for clustering.\n",
    "subset_size = 20000\n",
    "dataset = dataset.shuffle(seed=42).select(range(subset_size))\n",
    "\n",
    "texts = dataset[\"text\"]\n",
    "labels = dataset[\"label\"]\n",
    "label_names = dataset.features[\"label\"].names\n",
    "\n",
    "print(f\"Loaded {len(texts)} samples.\")\n",
    "print(f\"Example category: {label_names[labels[0]]}\")\n",
    "print(f\"Example text: {texts[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate vector embeddings for our text. We use `all-MiniLM-L6-v2` from SentenceTransformers, which is fast and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 1854.42it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings using all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:20<00:00, 30.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (20000, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"Generating embeddings using {model_name}...\")\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RQ-KMeans (Residual Quantization K-Means)\n",
    "\n",
    "This algorithm hierarchically clusters the data. \n",
    "- **Level 1**: Coarse clustering (e.g., broad topics)\n",
    "- **Level 2**: Fine clustering (sub-topics)\n",
    "- **Level 3+**: Even finer detail\n",
    "\n",
    "We will use **4 levels** with **10 clusters** each. This gives us a theoretical capacity of $10^4 = 10,000$ unique buckets. Since we have 20,000 items, we *will* have collisions, which we'll handle later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Mac MPS ðŸŽ\n",
      "Training RQ-KMeans...\n",
      "Training level 1/4 (K=10) on mps...\n",
      "Training level 2/4 (K=10) on mps...\n",
      "Training level 3/4 (K=10) on mps...\n",
      "Training level 4/4 (K=10) on mps...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<semantic_id.algorithms.rq_kmeans.RQKMeans at 0x107e68690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Using GPU (CUDA) ðŸš€\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Mac MPS ðŸŽ\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU ðŸ¢\")\n",
    "\n",
    "# Initialize RQ-KMeans\n",
    "rq_kmeans = RQKMeans(\n",
    "    n_levels=4, \n",
    "    n_clusters=10, \n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training RQ-KMeans...\")\n",
    "rq_kmeans.fit(embeddings, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Semantic IDs\n",
    "Now let's see what the IDs look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label semantic_id\n",
      "0  Bangladesh paralysed by strikes Opposition act...     World     7-5-8-3\n",
      "1  Desiring Stability Redskins coach Joe Gibbs ex...    Sports     1-1-2-3\n",
      "2  Will Putin #39;s Power Play Make Russia Safer?...     World     2-0-0-8\n",
      "3  U2 pitches for Apple New iTunes ads airing dur...  Sci/Tech     9-3-2-0\n",
      "4  S African TV in beheading blunder Public broad...     World     7-6-4-3\n",
      "5  A Cosmic Storm: When Galaxy Clusters Collide A...  Sci/Tech     3-9-6-4\n",
      "6  West sets deadline for Iran to freeze uranium ...     World     4-1-6-0\n",
      "7  Computer Assoc. Cuts 800 Jobs Worldwide (AP) A...  Sci/Tech     6-4-8-2\n",
      "8  CA Opens Utility Pricing for Mainframes Keepin...  Sci/Tech     6-1-5-2\n",
      "9  Economy builds steam in KC Fed district The ec...  Business     5-1-5-9\n"
     ]
    }
   ],
   "source": [
    "codes = rq_kmeans.encode(embeddings, device=device)\n",
    "sids = rq_kmeans.semantic_id(codes)\n",
    "\n",
    "# Create a DataFrame to analyze\n",
    "df = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"label\": [label_names[l] for l in labels],\n",
    "    \"semantic_id\": sids\n",
    "})\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Clustering Quality\n",
    "Let's sort by `semantic_id` to see if similar articles are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      semantic_id     label                                               text\n",
      "437       0-0-0-0  Sci/Tech  Secret Service Busts Cyber Gangs The US Secret...\n",
      "2321      0-0-0-0  Sci/Tech  ISA Server 2000 and Proxy Server 2.0 Internet ...\n",
      "2552      0-0-0-0  Sci/Tech  Group Seeks Ways to Prosecute Cybercrime Gover...\n",
      "2910      0-0-0-0  Sci/Tech  News: Terrorists grow fat on email scams Organ...\n",
      "3540      0-0-0-0     World  Poison porn pics show up online The first imag...\n",
      "4243      0-0-0-0  Sci/Tech  Possible security breach seen at AOL America O...\n",
      "5428      0-0-0-0  Sci/Tech  Dozens of Internet Crime Suspects Nabbed A sum...\n",
      "5889      0-0-0-0  Sci/Tech  'Phishing' scam targets NatWest NatWest bank s...\n",
      "6059      0-0-0-0  Sci/Tech  How Not to Get  #39;Phished #39; With phishing...\n",
      "6173      0-0-0-0  Sci/Tech  Banks Warned Against On-Line Fraud Dangers (Re...\n",
      "6914      0-0-0-0  Sci/Tech  First US Spam Conviction A man was convicted i...\n",
      "6915      0-0-0-0  Sci/Tech  Notice: IE  #39;worm #39; targets IOL ad suppl...\n",
      "7639      0-0-0-0  Business  eBay Goes Phishing The popular online auction ...\n",
      "7918      0-0-0-0  Sci/Tech  Judge Rejects Maryland Spam Law A Montgomery C...\n",
      "9002      0-0-0-0  Business  FDIC Warns About E-Mail 'Phishing' Scam (Reute...\n",
      "9548      0-0-0-0  Sci/Tech  All terror attacks use false passports, claims...\n",
      "10056     0-0-0-0  Sci/Tech  FDIC warns consumers on e-mail scams The FDIC ...\n",
      "10375     0-0-0-0  Sci/Tech  Bofra exploit hits our ad serving supplier &lt...\n",
      "10437     0-0-0-0  Sci/Tech  Lycos Europe denies attack on zombie army Inte...\n",
      "10462     0-0-0-0  Sci/Tech  Does Online Banking Put Your Money at Risk? Sc...\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort_values(\"semantic_id\")\n",
    "print(df_sorted[[\"semantic_id\", \"label\", \"text\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Uniqueness (The Engine)\n",
    "\n",
    "As predicted, we likely have duplicates (multiple items with the same ID like `3-9-1-0`). The `SemanticIdEngine` solves this by appending a counter.\n",
    "\n",
    "Example: \n",
    "- Item 1 -> `3-9-1-0`\n",
    "- Item 2 -> `3-9-1-0-1`\n",
    "- Item 3 -> `3-9-1-0-2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unique IDs...\n",
      "Total items with collision handling: 14801\n",
      "                                                  text  unique_id\n",
      "64   Moss officially out for Green Bay EDEN PRAIRIE...  1-6-2-0-1\n",
      "78   One last mile to go before they sweep Pedro Ma...  9-9-6-6-1\n",
      "89   Over 80 hurt on first day of Bangladesh strike...  7-5-4-2-1\n",
      "117  St. Louis Rams Release Punter Sean Landeta (AP...  1-6-2-0-2\n",
      "138  Having the final say FOXBOROUGH -- The Patriot...  1-0-6-7-1\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Collision Store (SQLite for persistence)\n",
    "# This saves the counters so if you restart the script, it remembers the next available suffix.\n",
    "store = SQLiteCollisionStore(\"ag_news_collisions.db\")\n",
    "\n",
    "# 2. Setup Resolver\n",
    "resolver = UniqueIdResolver(store=store)\n",
    "\n",
    "# 3. Create Engine\n",
    "engine = SemanticIdEngine(encoder=rq_kmeans, unique_resolver=resolver)\n",
    "\n",
    "# 4. Generate Unique IDs\n",
    "print(\"Generating unique IDs...\")\n",
    "unique_ids = engine.unique_ids(embeddings)\n",
    "\n",
    "df[\"unique_id\"] = unique_ids\n",
    "\n",
    "# Show examples where collisions occurred (ID length > standard 7 chars)\n",
    "collisions = df[df[\"unique_id\"].str.len() > 7]\n",
    "print(f\"Total items with collision handling: {len(collisions)}\")\n",
    "print(collisions[[\"text\", \"unique_id\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RQ-VAE (Neural Network Approach)\n",
    "\n",
    "For complex distributions, **RQ-VAE** (Residual Quantization Variational Autoencoder) can learn better representations than K-Means. It uses a neural network to project data into codebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RQ-VAE...\n",
      "Training RQ-VAE (5 epochs)...\n",
      "Epoch 1/5 - Loss: 0.0030 (Recon: 0.0025) - L0: 100.0% (Perp: 7.7) | L1: 87.5% (Perp: 11.9) | L2: 100.0% (Perp: 13.2) | L3: 87.5% (Perp: 10.8)\n",
      "Epoch 2/5 - Loss: 0.0028 (Recon: 0.0024) - L0: 100.0% (Perp: 9.3) | L1: 87.5% (Perp: 12.6) | L2: 100.0% (Perp: 14.2) | L3: 81.2% (Perp: 11.1)\n",
      "Epoch 3/5 - Loss: 0.0027 (Recon: 0.0024) - L0: 93.8% (Perp: 9.6) | L1: 87.5% (Perp: 13.1) | L2: 100.0% (Perp: 14.7) | L3: 81.2% (Perp: 11.7)\n",
      "Epoch 4/5 - Loss: 0.0026 (Recon: 0.0023) - L0: 93.8% (Perp: 9.8) | L1: 87.5% (Perp: 13.5) | L2: 100.0% (Perp: 14.6) | L3: 87.5% (Perp: 12.3)\n",
      "Epoch 5/5 - Loss: 0.0026 (Recon: 0.0023) - L0: 93.8% (Perp: 10.7) | L1: 93.8% (Perp: 13.7) | L2: 100.0% (Perp: 15.0) | L3: 93.8% (Perp: 12.8)\n",
      "Sample VAE ID: 6-11-3-11\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing RQ-VAE...\")\n",
    "\n",
    "rq_vae = RQVAE(\n",
    "    in_dim=embeddings.shape[1], # 384\n",
    "    num_emb_list=[16, 16, 16, 16], # 4 levels\n",
    "    e_dim=32,                   # Codebook dimension\n",
    "    layers=[128, 64],           # Encoder/Decoder layers\n",
    "    device=device,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    kmeans_init=True\n",
    ")\n",
    "\n",
    "print(\"Training RQ-VAE (5 epochs)...\")\n",
    "# Note: In a real scenario, train for more epochs!\n",
    "rq_vae.fit(embeddings)\n",
    "\n",
    "# Generate IDs\n",
    "vae_codes = rq_vae.encode(embeddings, device=device)\n",
    "vae_ids = rq_vae.semantic_id(vae_codes)\n",
    "\n",
    "print(f\"Sample VAE ID: {vae_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully:\n",
    "1.  Generated embeddings for text data.\n",
    "2.  Clustered them into **Semantic IDs** using RQ-KMeans.\n",
    "3.  Ensured every ID is unique using the **SemanticIdEngine**.\n",
    "4.  Experimented with **RQ-VAE** for neural quantization.\n",
    "\n",
    "Happy clustering! ðŸŒŸ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
